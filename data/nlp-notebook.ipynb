{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbdb2cd-4f0b-4f1e-80e4-0303d6638eba",
   "metadata": {},
   "source": [
    "Develop a Recurrent Neural Network Text Classification Model in Python. Showcase some of the NLP tools (you will likely do some of this by necessity for your model). Evaluate the model. Determine whether it is a good model. Include complete explanation of what you did, why you did it, the results, and your conclusion. Ideally I am looking for a somewhat broad NLP analysis, using some of the tools outlined in the week #7 files in addition to the RNN model. Make sure you include comments and show OUTPUTS, and upload the dataset, if applicable.  When in doubt give me more information than you think I need.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77ccac4-5b1a-4050-ad1e-f50ea6d4911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 22:27:00.976690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrickburke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e34ded3-47bb-4480-8022-d1e0bbc22376",
   "metadata": {},
   "source": [
    "For this assignment, we will be utilizing the women's clothing customer review dataset found on Kaggle (https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews) to determine if, given a customer's review, we can predict if a customer would recommend the product or not. We are specifically interested in how well we can predict if a customer would **not** recommend a specific product (Recommended IND = 0), and will attempt to create a model with this aspect in mind.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec4f8e7-c7e1-493f-bb69-d2f06f9df32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23486"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the clothing review dataset \n",
    "clothing_data = pd.read_csv(\n",
    "    'clothing_reviews.csv', \n",
    "    usecols=['Review Text', 'Recommended IND', 'Rating'], \n",
    "    dtype={'Review Text': str, 'Recommended IND': np.int64, 'Rating': np.int64}\n",
    ")\n",
    "clothing_data.rename(columns={\"Review Text\": \"Text\"}, inplace=True)\n",
    "clothing_data.rename(columns={\"Recommended IND\": \"Would Recommend\"}, inplace=True)\n",
    "\n",
    "len(clothing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d564d3ea-fe4a-4a4d-8ec2-f4206543e4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8223622583666865"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate total ratio of 'Would Recommend' to 'Would Not Recommend'\n",
    "sum(clothing_data['Would Recommend'])/len(clothing_data['Would Recommend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c1f76e-ac04-45b9-a5e3-b993cb5c2cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Would Recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Rating  Would Recommend\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1\n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1\n",
       "2  I had such high hopes for this dress and reall...       3                0\n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1\n",
       "4  This shirt is very flattering to all due to th...       5                1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a snapshow of what our data looks like\n",
    "clothing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4cee5e-118c-47ae-a396-a79f0bce70cc",
   "metadata": {},
   "source": [
    "We will first import the necessary variables (the review text & if the customer would recommend the piece or not), along with the rating, which we may be able to use as validation to see if another model may be more useful. We will then split the data into training, validation, and testing datasets, with 20% of the original data being used as test data and 20% of the training set being used as validation data. We can see from our import that there are a total of 23,486 reviews, with 82.23% of them resulting in a 'Would Recommend' rating (with 0 = Would Not Recommend, 1 = Would Recommend). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06de60d2-9a92-4ee9-b0cc-c4c9110cc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clothing_data['Text']\n",
    "y = clothing_data['Would Recommend']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 123)  \n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state = 123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ae4daa-e3b4-47d5-8a9d-435797232498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13230    Super easy and cute. i received lots of compli...\n",
       "4644     I really love this top. it looks great under a...\n",
       "5992     I purchased the teal/blue version and the colo...\n",
       "1791     I bought the grey and white plaid shirt in a l...\n",
       "13652    Beautiful detail. sweater is warm, soft, and v...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b5955f7-5975-4e52-b084-8b380aab10a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8216899534264803"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9df1c1-ec23-4f1f-80db-18b8d7c149f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.810803618946248"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_val)/len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074b4624-2542-411d-953a-1a0ebd19b524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8337590464027246"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794c422e-2b2b-4d0a-b67e-a656990fe3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Would Recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super easy and cute. i received lots of compli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I really love this top. it looks great under a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I purchased the teal/blue version and the colo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought the grey and white plaid shirt in a l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beautiful detail. sweater is warm, soft, and v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Would Recommend\n",
       "0  Super easy and cute. i received lots of compli...                1\n",
       "1  I really love this top. it looks great under a...                1\n",
       "2  I purchased the teal/blue version and the colo...                1\n",
       "3  I bought the grey and white plaid shirt in a l...                1\n",
       "4  Beautiful detail. sweater is warm, soft, and v...                1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.column_stack((x_train, y_train))\n",
    "train_data = pd.DataFrame(train_data, columns = ['Text','Would Recommend'])\n",
    "\n",
    "# make sure to reset data to necessary type\n",
    "type_dict = {'Text': str,\n",
    "             'Would Recommend': np.int64\n",
    "                }\n",
    " \n",
    "train_data = train_data.astype(type_dict)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a32b90-1840-4b40-8d26-f72546bb08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = np.column_stack((x_val, y_val))\n",
    "val_data = pd.DataFrame(val_data, columns = ['Text','Would Recommend'])\n",
    "val_data = val_data.astype(type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1966f1b0-68fd-4b83-a00c-f1e1d5a60b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.column_stack((x_test, y_test))\n",
    "test_data = pd.DataFrame(test_data, columns = ['Text','Would Recommend'])\n",
    "test_data = test_data.astype(type_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7694c8-d4bc-4b7e-bccf-28e2e353f7de",
   "metadata": {},
   "source": [
    "We will next clean our text data, to remove any unwanted noise from things like urls or punctuation (done through TensorFlow tokenizer). We will also remove stop words like 'a' or 'the' that will not add meaning into our final model. Finally, we will revert the words in our data to their stem, or base derivation, which may help improve our model further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef68ebaa-9465-4f8b-b65c-319bb009d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls from text\n",
    "def remove_url(sentence):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', sentence)\n",
    "\n",
    "# remove any emojis from text\n",
    "def remove_emoji(sentence):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', sentence)\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(sentence):\n",
    "    words = sentence.split()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# \n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_words(sentence):\n",
    "    words = sentence.split()\n",
    "    words = [stemmer.stem(word) for word in words ]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_text(data):\n",
    "    data['Text'] = data['Text'].apply(lambda x : remove_url(x))\n",
    "    data['Text'] = data['Text'].apply(lambda x : remove_emoji(x))\n",
    "    data['Text'] = data['Text'].apply(lambda x : remove_stopwords(x))\n",
    "    data['Text'] = data['Text'].apply(lambda x : stem_words(x))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e5debf1-72c1-43cc-8f59-e035fda00faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text of our training, validation, and testing data\n",
    "train_data = clean_text(train_data)\n",
    "val_data = clean_text(val_data)\n",
    "test_data = clean_text(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf91c60e-66f4-4a7a-80fa-bc47b4ec1ad2",
   "metadata": {},
   "source": [
    "After cleaning our data, we will now tokenize each of the text values so that our model can understand our input properly. To do this, we will utilise two functions that will assign index numbers to each word in the dataset, then encode the sentences so that each word is represented with an indexnumber in an array of index numbers, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc4fbc0c-f9ff-438d-a76f-c2e6c027bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to assign index numbers to each word\n",
    "def define_tokenizer(train_sentences, val_sentences, test_sentences):\n",
    "    sentences = pd.concat([train_sentences, test_sentences])\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# function used to encode each review into an array of index numbers\n",
    "def encode(sentences, tokenizer):\n",
    "    encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "    encoded_sentences = tf.keras.preprocessing.sequence.pad_sequences(encoded_sentences)\n",
    "    \n",
    "    return encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea344d3b-420e-46c6-a1dd-b3fed6a36c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and encode our text data\n",
    "tokenizer = define_tokenizer(train_data['Text'], val_data['Text'], test_data['Text'])\n",
    "\n",
    "encoded_train_reviews = encode(train_data['Text'], tokenizer)\n",
    "encoded_val_reviews = encode(val_data['Text'], tokenizer)\n",
    "encoded_test_reviews = encode(test_data['Text'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e512982-7554-4ffb-a0ea-c0ed1a715414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12318"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words in encoding dictionary\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1d6762f-475d-415d-b2f6-6bd9519d1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower:  True\n",
      "Split:   \n",
      "Filters:  !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view general information from tokenizer \n",
    "print('Lower: ', tokenizer.get_config()['lower'])\n",
    "print('Split: ', tokenizer.get_config()['split'])\n",
    "print('Filters: ', tokenizer.get_config()['filters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e644e-f421-498b-ad4b-44efd0089067",
   "metadata": {},
   "source": [
    "To make sure our model trains on a more complete array dataset, we will import the GloVe Embedding and synchronize this embedding with our own encoding in order to have a more complete dataset for our model to train on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06f0d11e-428a-49d7-935a-7e60362316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "\n",
    "with open('glove.6B.100d.txt','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word] = vectors\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad2b1886-a751-4d22-bf61-29f361137138",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec = embedding_dict.get(word)\n",
    "    \n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f79dc1-399a-4af8-803e-05da89d5eaf5",
   "metadata": {},
   "source": [
    "We will now make sure to convert our text data into the native TensorFlow data format, so that we can maximize TensorFlow functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c726c402-6cb1-4d47-9e5d-243d21d84555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 22:36:52.949675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# convert training data into native TensorFlow format\n",
    "tf_data = tf.data.Dataset.from_tensor_slices((encoded_train_reviews, train_data['Would Recommend'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdafea-f184-4507-97c9-1c2f76e6e4b6",
   "metadata": {},
   "source": [
    "We will now define the pipeline for our reformatted training data, along with reformatting and defining a pipeline for our validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7139cea-0a0b-4b77-a5a2-86a8f401cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "def pipeline(tf_data, buffer_size=100, batch_size=32):\n",
    "    tf_data = tf_data.shuffle(buffer_size)    \n",
    "    tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    tf_data = tf_data.padded_batch(batch_size, padded_shapes=([None],[]))\n",
    "    \n",
    "    return tf_data\n",
    "\n",
    "tf_data = pipeline(tf_data, buffer_size=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaf978b5-cc68-4c13-aab9-70bca00f44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert validation data to native TensorFlow format\n",
    "tf_val_data = tf.data.Dataset.from_tensor_slices((encoded_val_reviews, val_data['Would Recommend'].values))\n",
    "\n",
    "# define pipeline\n",
    "def val_pipeline(tf_data, batch_size=1):        \n",
    "    tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    tf_data = tf_data.padded_batch(batch_size, padded_shapes=([None],[]))\n",
    "    \n",
    "    return tf_data\n",
    "\n",
    "tf_val_data = val_pipeline(tf_val_data, batch_size=len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66bbe45-848b-42a8-9472-afbe36d39c86",
   "metadata": {},
   "source": [
    "After reformatting our data, we can now define our model. In this case, we will first define an embedding layer so that our model can gain an understanding of a words meaning, then an RNN layer so that our model can begin to build relationships between words. Finally, we will use our Dense layer to output if our text indicates a recommendation or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25597fdb-5b99-49b0-a6c0-1441eb672a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the embedding, RNN & Dense layers of the model, using LSTM as our RNN model of choice in this instance\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    len(tokenizer.word_index) + 1,\n",
    "    100,\n",
    "    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable = True\n",
    ")\n",
    "model = tf.keras.Sequential([\n",
    "    embedding,\n",
    "    tf.keras.layers.SpatialDropout1D(0.2),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be4dd10e-73cc-4925-b7a4-f032a32c3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "    metrics=['accuracy', 'Precision', 'Recall']\n",
    ")\n",
    "\n",
    "# make sure to avoid stepping past optimum\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=4, verbose=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097b5b1-f872-4108-ba69-2903a2a2c826",
   "metadata": {},
   "source": [
    "After defining and compiling our model, we will now fit our model over 10 epochs on our training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1a8064a-32d0-48da-b1c0-20807f2336cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "470/470 [==============================] - 118s 239ms/step - loss: 0.4609 - accuracy: 0.8146 - precision: 0.8219 - recall: 0.9885 - val_loss: 0.4149 - val_accuracy: 0.8132 - val_precision: 0.8175 - val_recall: 0.9908 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "470/470 [==============================] - 108s 231ms/step - loss: 0.3832 - accuracy: 0.8338 - precision: 0.8403 - recall: 0.9849 - val_loss: 0.3903 - val_accuracy: 0.8297 - val_precision: 0.8389 - val_recall: 0.9777 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "470/470 [==============================] - 108s 230ms/step - loss: 0.3558 - accuracy: 0.8444 - precision: 0.8603 - recall: 0.9677 - val_loss: 0.3579 - val_accuracy: 0.8454 - val_precision: 0.8539 - val_recall: 0.9764 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "470/470 [==============================] - 108s 229ms/step - loss: 0.3349 - accuracy: 0.8533 - precision: 0.8741 - recall: 0.9597 - val_loss: 0.3282 - val_accuracy: 0.8536 - val_precision: 0.8735 - val_recall: 0.9583 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "470/470 [==============================] - 107s 228ms/step - loss: 0.3150 - accuracy: 0.8621 - precision: 0.8856 - recall: 0.9555 - val_loss: 0.3227 - val_accuracy: 0.8598 - val_precision: 0.8750 - val_recall: 0.9649 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "470/470 [==============================] - 117s 249ms/step - loss: 0.3011 - accuracy: 0.8714 - precision: 0.8958 - recall: 0.9546 - val_loss: 0.3278 - val_accuracy: 0.8608 - val_precision: 0.8688 - val_recall: 0.9757 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "470/470 [==============================] - 113s 241ms/step - loss: 0.2896 - accuracy: 0.8772 - precision: 0.9017 - recall: 0.9547 - val_loss: 0.2962 - val_accuracy: 0.8749 - val_precision: 0.9081 - val_recall: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "470/470 [==============================] - 126s 268ms/step - loss: 0.2795 - accuracy: 0.8802 - precision: 0.9063 - recall: 0.9528 - val_loss: 0.2899 - val_accuracy: 0.8768 - val_precision: 0.9144 - val_recall: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "470/470 [==============================] - 117s 250ms/step - loss: 0.2734 - accuracy: 0.8839 - precision: 0.9085 - recall: 0.9548 - val_loss: 0.2891 - val_accuracy: 0.8771 - val_precision: 0.9264 - val_recall: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "470/470 [==============================] - 115s 244ms/step - loss: 0.2625 - accuracy: 0.8892 - precision: 0.9158 - recall: 0.9528 - val_loss: 0.2895 - val_accuracy: 0.8765 - val_precision: 0.8905 - val_recall: 0.9665 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd0fd5d7c0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_data, \n",
    "    validation_data = tf_val_data,\n",
    "    epochs = 10,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38f8cf-6d46-447e-a6c8-9dc1e4406f0f",
   "metadata": {},
   "source": [
    "To evaluate our model, we will use the model F1 score on our validation data, to check that our model is perfoming adequately before testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b63b1c61-939a-4611-943b-0d1c72ffca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.2895 - accuracy: 0.8765 - precision: 0.8905 - recall: 0.9665\n",
      "F1 score: 0.9269751246928085\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(tf_val_data)\n",
    "\n",
    "precision = metrics[2]\n",
    "recall = metrics[3]\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print('F1 score: ' + str(f1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf09568-06c8-464a-8a72-f6090d9c7c97",
   "metadata": {},
   "source": [
    "From our F1 score of 0.927, we can be fairly confident that our model will perform well with our test data. We will now create a new pipeline definition for our test data, then use our model to predict if the review implies a potential recommendation or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b15a4402-4a9a-4691-bfab-c08e53fdd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_data = tf.data.Dataset.from_tensor_slices((encoded_test_reviews))\n",
    "\n",
    "def test_pipeline(tf_data, batch_size=1):        \n",
    "    tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    tf_data = tf_data.padded_batch(batch_size, padded_shapes=([None]))\n",
    "    \n",
    "    return tf_data\n",
    "\n",
    "tf_test_data = test_pipeline(tf_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd56d1a6-57b4-4734-b1b2-a6b50946b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4698/4698 [==============================] - 76s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(tf_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19cb3483-90ae-411e-a82a-b90b975f8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5774391e-ff92-45a7-a086-689ae3914263",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = np.column_stack((y_test, predictions))\n",
    "compare = pd.DataFrame(compare, columns = ['Test Recommendation','Predicted Recommendation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5bd4c8e-9e94-4c96-83d3-5504558816b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888250319284802"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine percentage of correct predictions\n",
    "len(compare.query('`Test Recommendation` == `Predicted Recommendation`'))/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c559b8ea-fa92-4379-b3e4-6aa56a351dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7466666666666667"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine ratio of false positives to total incorrect predictions\n",
    "incorrect_prediction = compare.query('`Test Recommendation` != `Predicted Recommendation`')\n",
    "false_recommendation = incorrect_prediction.query('`Test Recommendation` == 0')   \n",
    "len(false_recommendation)/len(incorrect_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aab6e8-feae-45d2-aa79-52e34f049822",
   "metadata": {},
   "source": [
    "From our predictions, we can see that we have correctly predicted about 88.25% of the total number of test reviews, which in most cases is extremely high and shows a useful model. However, we can see that of our incorrect predictions, about 74.67% of them are of incorrectly predicting that a customer would recommend a piece of clothing when they would not. This means that, while we have an overall fairly accurate model, a lot of our accuracy comes from the dramatic skew in our data towards positive recommendations. Therefore, while our model does predict better than average ( ~ 80% from guessing with respects to the predetermined ratios), we can still see potential improvements in this model. One such potential improvement is to determine a way to adequately weigh the negative reviews higher, without creating unwanted bias or adding negative sentiment to words that may not necessarly call for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a37266-e734-4ae1-b84d-5a5b91177273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
